{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment of Typewise and ChatGPT on their GED Performance\n",
        "\n",
        "This notebook can be used to assess the grammatical error detecion performance of Typewise and ChatGPT using the [Corpus DYS](https://nakala.fr/10.34847/nkl.ced0370u) of French dyslexic texts [(Bodard et al. 2022)](https://link.springer.com/10.1007/s10579-022-09603-6). It computes Precision/Recall/F1/Accuracy and counts the missed errors per error type for each system.\n",
        "\n"
      ],
      "metadata": {
        "id": "L1-MzagkRWYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb"
      ],
      "metadata": {
        "id": "G7peGu85-z-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123ad8d9-f5fc-4bf4-a963-6f4477be83f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.8/775.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "129R5IeOnZDL",
        "outputId": "85a166e5-d07c-4835-e5aa-cded4f3d793e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sacremoses) (4.64.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d79cef30f88e7dfb297f252ed0f50659143b55fe457e17c5b33d3174ff416397\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import ipdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sacremoses import MosesTokenizer\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "8W-_WabCNa7V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "### Utility Classes and Functions for Data Preparation"
      ],
      "metadata": {
        "id": "hcsy19LGKYRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IndexedToken():\n",
        "\n",
        "  def __init__(self, token, offset, length, label=None, types=None):\n",
        "    self.token = token\n",
        "    self.char_offset = offset\n",
        "    self.length = length\n",
        "    self.label = label\n",
        "    self.types = types\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str((self.token, self.char_offset, self.length, self.label))\n",
        "\n",
        "  def set_label(self, label):\n",
        "    self.label = label\n",
        "\n",
        "  def set_types(self, types):\n",
        "    self.types = types\n",
        "\n",
        "class TokenizedSegment():\n",
        "\n",
        "  def __init__(self, tokens):\n",
        "    self.indexed_tokens = tokens\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.indexed_tokens)\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return TokenizedSegmentIter(self)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.indexed_tokens[index]\n",
        "\n",
        "\n",
        "\n",
        "class TokenizedSegmentIter():\n",
        "  def __init__(self, tokenized_segment):\n",
        "    self._indexed_tokens = tokenized_segment.indexed_tokens\n",
        "    self._current_index = 0\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    if self._current_index < len(self._indexed_tokens):\n",
        "      token = self._indexed_tokens[self._current_index]\n",
        "      self._current_index += 1\n",
        "      return token\n",
        "    raise StopIteration"
      ],
      "metadata": {
        "id": "9NoKgH1mtsqe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_offsets(sentence):\n",
        "    tokenizer = MosesTokenizer()\n",
        "    tokens = tokenizer.tokenize(sentence.strip())\n",
        "    token_offsets = []\n",
        "    current_offset = 0\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_offset = IndexedToken(token, current_offset, len(token))\n",
        "        token_offsets.append(token_offset)\n",
        "        current_offset += len(token)\n",
        "        while (current_offset + 1) < len(sentence) and sentence[current_offset].isspace():\n",
        "          current_offset += 1\n",
        "    tokenized_sent = TokenizedSegment(token_offsets)\n",
        "    return tokenized_sent"
      ],
      "metadata": {
        "id": "DDzCvskrnBue"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gold Data"
      ],
      "metadata": {
        "id": "gREZTUbyMX9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if an error form contains multiple error types, there are missing values in the csv\n",
        "\n",
        "def fill_missing_values(infile, outfile):\n",
        "    rows = []\n",
        "    # Open the CSV file and read the rows\n",
        "    with open(infile, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            rows.append(row)\n",
        "    # Iterate through the rows and fill in missing values\n",
        "    for i, row in enumerate(rows):\n",
        "        # leave incomplete rows because of omissions unchanged\n",
        "        if not row[1] and row[2]:\n",
        "          continue\n",
        "        # leave incomplete rows because of additions unchanged\n",
        "        elif row[1] and not row[2]:\n",
        "          continue\n",
        "        # fill in missing values if multiple types are annotated for an error\n",
        "        elif not row[1] and not row[2] and not row[3]:\n",
        "          for j, value in enumerate(row):\n",
        "            if not value:\n",
        "              row[j] = rows[i-1][j]\n",
        "    # Write the updated rows back to the CSV file\n",
        "    with open(outfile, 'w', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for row in rows:\n",
        "            writer.writerow(row)"
      ],
      "metadata": {
        "id": "g5jg8oF2MNvI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "apPbFL8x3lUy"
      },
      "outputs": [],
      "source": [
        "def clean_annotated_segment(segment):\n",
        "  # remove annotated word omissions\n",
        "  segment = re.sub(r\"<err>\\[.+\\]</err>\\s?\", \"\", segment)\n",
        "  # remove err tags\n",
        "  segment = segment.replace(\"<err>\", \"\").replace(\"</err>\", \"\")\n",
        "\n",
        "  return segment\n",
        "\n",
        "\n",
        "def read_gold_data(path):\n",
        "  df = pd.read_csv(path)\n",
        "\n",
        "  # Create a new column with the cleaned sentences\n",
        "  df[\"Phrase ou syntagme cleaned\"] = df[\"Phrase ou syntagme\"].apply(lambda x: clean_annotated_segment(x))\n",
        "\n",
        "  # Group the dataframe by the cleaned sentences and keep all the original rows\n",
        "  grouped = df.groupby(\"Phrase ou syntagme cleaned\")\n",
        "\n",
        "  merged_rows = []\n",
        "\n",
        "  for name, group in grouped:\n",
        "      errors = []\n",
        "\n",
        "      for index, row in group.iterrows():\n",
        "          # Get the original sentence and the error\n",
        "          sentence = row[\"Phrase ou syntagme cleaned\"].strip()\n",
        "          annotated_sentence = row[\"Phrase ou syntagme\"].strip()\n",
        "\n",
        "          error = row[\"Forme erronée\"] if row[\"Forme erronée\"] else \"\"        \n",
        "\n",
        "          # Find the character offset and end indexes of the error in the sentence\n",
        "          if row[\"Type de l'erreur\"] == \"omission mot\":\n",
        "            correction = row[\"Forme correcte\"]\n",
        "            try:\n",
        "              start = annotated_sentence.index(f\"<err>[{correction.strip()}]</err>\")\n",
        "              end = start\n",
        "            except AttributeError:\n",
        "              continue\n",
        "          else:\n",
        "            start = sentence.index(f\"{error.strip()}\")\n",
        "            end = start + len(f\"{error.strip()}\")\n",
        "          \n",
        "          errors.append({\n",
        "              \"offset\": start,\n",
        "              \"length\": end - start,\n",
        "              \"type\": row[\"Type de l'erreur\"],\n",
        "              \"error_form\": error\n",
        "          })\n",
        "\n",
        "      merged_rows.append({\n",
        "          \"sentence\": sentence,\n",
        "          \"errors\": errors\n",
        "      })\n",
        "  return merged_rows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fill_missing_values(\"annotation_erreurs_corpus_dys.csv\", \"annotation_erreurs_corpus_dys_filled.csv\")\n",
        "\n",
        "segments = read_gold_data(\"annotation_erreurs_corpus_dys_filled.csv\")\n",
        "\n",
        "train, test = train_test_split(segments, test_size=0.4, random_state=42)\n",
        "\n",
        "with open(\"gold_data_test.jsonl\", \"w\", encoding=\"utf-8\") as outf:\n",
        "  for seg in test:\n",
        "    json_line = json.dumps(seg)\n",
        "    outf.write(f\"{json_line}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zysr3ZwQwMKX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare gold data for comparison with system outputs\n",
        "\n",
        "def prepare_gold_data(data):\n",
        "  gold_data = []\n",
        "  for segment in data:\n",
        "    tokenized_sent = tokenize_with_offsets(segment[\"sentence\"])\n",
        "\n",
        "    error_offsets = [error[\"offset\"] for error in segment[\"errors\"]]\n",
        "\n",
        "    error_tokens = [error[\"error_form\"] for error in segment[\"errors\"]]\n",
        "\n",
        "    tokens = [tok.token for tok in tokenized_sent]\n",
        "\n",
        "    for err_tok in error_tokens:\n",
        "      if err_tok not in tokens:\n",
        "        continue\n",
        "    errors = {}\n",
        "    for error in segment[\"errors\"]:\n",
        "      if error[\"offset\"] in errors.keys():\n",
        "        errors[error[\"offset\"]].append(error[\"type\"])\n",
        "      else:\n",
        "        errors[error[\"offset\"]] = [error[\"type\"]] \n",
        "    for i, token in enumerate(tokenized_sent):\n",
        "        if token.char_offset in errors.keys():\n",
        "          tokenized_sent[i].set_label(\"err\")\n",
        "          tokenized_sent[i].set_types(errors[token.char_offset])\n",
        "        else:\n",
        "          tokenized_sent[i].set_label(\"corr\")\n",
        "    \n",
        "    gold_data.append(tokenized_sent)\n",
        "\n",
        "  return gold_data\n"
      ],
      "metadata": {
        "id": "33aHWfdWH7TF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typewise Data"
      ],
      "metadata": {
        "id": "LPXC1vsbMj-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Typewise API to annotate test data and dump the results to json files\n",
        "\n",
        "def annotate_with_typewise(data, correction_outfile, grammar_outfile):\n",
        "\n",
        "  annotated_data_correction = []\n",
        "  annotated_data_grammar = []\n",
        "\n",
        "\n",
        "  correction_url = \"https://api.typewise.ai/latest/correction/whole_sentence\"\n",
        "  grammar_correction_url = \"https://api.typewise.ai/latest/grammar_correction/whole_text_grammar_correction\"\n",
        "  \n",
        "  for segment in data:\n",
        "\n",
        "    payload = {\"languages\": [\"fr\"],\n",
        "               \"text\": segment[\"sentence\"].strip(),\n",
        "               \"remove_low_prob_tokens\": False}\n",
        "\n",
        "    correction_response = requests.post(correction_url, json=payload).json()\n",
        "    grammar_response = requests.post(grammar_correction_url, json=payload).json()\n",
        "\n",
        "    annotated_data_correction.append(correction_response)\n",
        "    annotated_data_grammar.append(grammar_response)\n",
        "  \n",
        "  correction = {\"responses\": annotated_data_correction}\n",
        "  grammar = {\"responses\": annotated_data_grammar}\n",
        "\n",
        "\n",
        "  with open(correction_outfile, 'w', encoding='utf-8') as corr, open(grammar_outfile, 'w', encoding='utf-8') as gram:\n",
        "    json.dump(correction, corr, indent=2)\n",
        "    json.dump(grammar, gram, indent=2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qmvtyus7Tgso"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare annotated data for comparison with gold data\n",
        "\n",
        "def prepare_annotations_typewise(correction_path, grammar_path):\n",
        "  with open(correction_path, 'r', encoding='utf-8') as correction, open(grammar_path, 'r', encoding='utf-8') as grammar:\n",
        "    correction_data = json.load(correction)\n",
        "    grammar_data = json.load(grammar)\n",
        "\n",
        "  annotated_data = []\n",
        "  for correction_response, grammar_response in zip(correction_data[\"responses\"], grammar_data[\"responses\"]):\n",
        "\n",
        "    error_offsets = [match[\"start_index\"] for match in correction_response[\"tokens\"]]\n",
        "    error_offsets.extend([match[\"startIndex\"] for match in grammar_response[\"matches\"]])\n",
        "\n",
        "    tokenized_sent = tokenize_with_offsets(correction_response[\"original_text\"].strip())\n",
        "\n",
        "    # for each IndexedToken, if offset is in error_offsets, set the label to \"err\"\n",
        "    for i, token in enumerate(tokenized_sent):\n",
        "      if token.char_offset in error_offsets:\n",
        "        tokenized_sent[i].set_label(\"err\")\n",
        "      else:\n",
        "        tokenized_sent[i].set_label(\"corr\")\n",
        "    \n",
        "    annotated_data.append(tokenized_sent)\n",
        "\n",
        "  return annotated_data"
      ],
      "metadata": {
        "id": "-d4l3sN1WKPk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only to showcase how it was annotated, data for reproduction are in the /data folder\n",
        "annotate_with_typewise(test, 'data/typewise_annotated_correction.json', 'data/typewise_annotated_grammar.json')"
      ],
      "metadata": {
        "id": "Rx_aiS6_UPLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT Data"
      ],
      "metadata": {
        "id": "o2VtmjDvN4nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print test data for ChatGPT\n",
        "\n",
        "i = 0\n",
        "\n",
        "for segment in test:\n",
        "  if (i % 2) == 0:\n",
        "    print(\"\\n\\n\")\n",
        "  print(segment[\"sentence\"])\n",
        "  i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUycqsG7n6Wc",
        "outputId": "f47e5cd8-3f27-4087-b605-64f324b8e086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "et tous le monde été choqué car je fesé tous se que je voulais\n",
            "puis au fur et mesure des années a cause de l’exode rural l’industrie tertiaire est devenu très importante ainsi réduisent le primaire a seulement 3 pourcent de la population actives\n",
            "\n",
            "\n",
            "\n",
            "il s’arrete sur le bas côté\n",
            "tu rigol toi qui ne veu aps rété avec té grenparen qui n'on que 65 ans tu ne va pas me dir que tu est en retar de troi heur  a ceuse d'une pérsone rencontré dans la rue comme sa\n",
            "\n",
            "\n",
            "\n",
            "puis j’ai une semaine de vacance et je risque d’aller faire du velo.\n",
            "Suite a pluseurs  rechercher et remise en quetion ! sur ma situiation.\n",
            "\n",
            "\n",
            "\n",
            "L'on pouré mettre en cause les professeur qui dé le colége dégoûte beaucoup d'élève ,\n",
            "dacor dacor alé raconte la moi séte istoire\n",
            "\n",
            "\n",
            "\n",
            "si microsoft  baise le prix de ses pack xbox 360 ses car la demende en frence de set consol est peu inportente donc le prix baiss\n",
            "pour prepare un cheval il faut un cur sabot , une brosse dure , une brosse bousse , pine , et equipement du cheval c est un tapi un files et un selle\n",
            "\n",
            "\n",
            "\n",
            "Pour finir l’ablation des hais dans les champs par le remembrement a permie d’augmenter leur taille a 50 hectar\n",
            "quand j’ai fini jai pri la voiture pour aller au rondevous\n",
            "\n",
            "\n",
            "\n",
            "le nombre de voiture imatriculer chute\n",
            "le prix quilibre est  dans le cas ou l'ofre et la demande sot identique\n",
            "\n",
            "\n",
            "\n",
            "toute foix l'on peu dir que une afair priver peu étre « étsalet dans le cas ou la persone a qui set afair priver touche directemen\n",
            "quel partie de l’application été impacté.\n",
            "\n",
            "\n",
            "\n",
            "que seului qui brime la violence de façon si flagrante en est et sera lui-même victime\n",
            "le japon a donc un éxédant inporten\n",
            "\n",
            "\n",
            "\n",
            "opel baisse ses prix car la demande sur le marché est devenu faible\n",
            "pour montrer que les choses avances\n",
            "\n",
            "\n",
            "\n",
            "oui j'ai édé une viéieu dame qui voulé plenté un arbre or de la ville qui l'étoufé\n",
            "Je reste peroide que il fautra du temps\n",
            "\n",
            "\n",
            "\n",
            "une presonne  reve de partie en vacances .\n",
            "J’espère que nous nous revérrons bientôt.\n",
            "\n",
            "\n",
            "\n",
            "et le plus gros probleme ses dans les mer car les animeaux marin meurt\n",
            "dimanche je vais aller voir mon papi a lhopital voir comment il va\n",
            "\n",
            "\n",
            "\n",
            "mon père etais présant et j’avais des bonne nouvelle a annocer !\n",
            "Le monsieur charge sa voiture pour partir en vacance à la mer.\n",
            "\n",
            "\n",
            "\n",
            "apré sony a en tous 77 usi réparti dans le monde entier qui ravitalle le japon les eta-uni l'europe\n",
            "sauf que moi voiture et en panne\n",
            "\n",
            "\n",
            "\n",
            "on peu donc dir que le japon et ne tré bonne situation économique\n",
            "je vai mettre dans la voiture\n",
            "\n",
            "\n",
            "\n",
            "sela peu venir de plusier  de chose bien que je panse que la majeur partie des cause sont due o sistéme scolaire élitiste français ,\n",
            "pendant tous l’été on ai rester a la maison avec ma copine.\n",
            "\n",
            "\n",
            "\n",
            "pour l’instent nous habition chez mes parent\n",
            "et la elle et entraind de faire une radio.\n",
            "\n",
            "\n",
            "\n",
            "Un monsieur se prepare a partir en vacance,\n",
            "les animaux  mange des sac en plastique  il sétoufe et il meurt\n",
            "\n",
            "\n",
            "\n",
            "je suit party me chancher\n",
            "tableau remplie sur feuille\n",
            "\n",
            "\n",
            "\n",
            "Je me préparer  ma valise pour partir en vacances.\n",
            "influenser ou coronpre\n",
            "\n",
            "\n",
            "\n",
            "pui le deuxiem architékte construi un mure\n",
            "Qustion 1) les région qui on une sécurité alimentaire sont la région sud et nor de l'Afrique *nsi que le Nigeria le camerun le Gabon le Togo et le senegal\n",
            "\n",
            "\n",
            "\n",
            "mais au bout d’un moman le moteur fume.\n",
            "des sac  en plastique que ca touche les mer les oceans le dessert le pole sur et le pole nord\n",
            "\n",
            "\n",
            "\n",
            "sété listoir de bouddah est de deux archi ...\n",
            "En rentrentde vacance il se posse plaint de question.\n",
            "\n",
            "\n",
            "\n",
            "se né pas son seul probléme\n",
            "va tu méxpliqué se que tu a trouvé d'intérésen a son aproche\n",
            "\n",
            "\n",
            "\n",
            "et le nombre d’exploitation et passer de 1 million a 500 mile mais leur taille a tout de même augmenter ainsi passent de 30 hectare a 50 hectare\n",
            "on na preparer les soutie  et nous sommes partie au boulo\n",
            "\n",
            "\n",
            "\n",
            "et donc « étoufer » une afair publique va l'encontr du droit des droit des media libre\n",
            "hier  jais rentre des chevaux pour les cour mon n’a noyer les écuries.\n",
            "\n",
            "\n",
            "\n",
            "les sac en plastiques sont des element qui polue beaucoups les oceans,\n",
            "les magasin m en donnerais plus il faudra les acheter en tisu\n",
            "\n",
            "\n",
            "\n",
            "apres de telle traitemenr la terre devien de plus en plus dure a cultiver et les plantation sont de moins en moins fast\n",
            "un manifestent avec un imanse pano\n",
            "\n",
            "\n",
            "\n",
            "elle éme les pon masi pas les mure\n",
            "m'ont frere va au lycee et ma sœur vas au ce1\n",
            "\n",
            "\n",
            "\n",
            "il et énerve  a causse de la panne de sa voiture.\n",
            "je suis allez donner le foin au chevaux.\n",
            "\n",
            "\n",
            "\n",
            "c'etait une femme coltiver toujour de bonne humeur\n",
            "mes frères et mon oncle on bu un petit rosé.\n",
            "\n",
            "\n",
            "\n",
            "contrôle d'éco-gestion\n",
            "tous dabor o nivo de larmemen il na pas les bonbe nucléére et ne peu pas avoir darmé pour insdir il peu pas non plus fair la geurre\n",
            "\n",
            "\n",
            "\n",
            "par contre elle éme les pont\n",
            "Les principal cause de l'inscurtité alimentaire sont la shéseraise et la charté des produits alimentaire\n",
            "\n",
            "\n",
            "\n",
            "d inde coute ma voiture fume\n",
            "se qui veut simplement dir que tout le monde est cnosérner  par sé divers l'arsin\n",
            "\n",
            "\n",
            "\n",
            "je me permet de vous donnée suite pour connaître l’avancer de vos démarches afin de prévoir une nouvelle collaboration enssemble.\n",
            "Il prépara alors ces bagages.\n",
            "\n",
            "\n",
            "\n",
            "il reculer pour avoir le cadre, mais derrier luiil avais des fleur et il tombe dedans.\n",
            "séte maude na pas de sanse ,\n",
            "\n",
            "\n",
            "\n",
            "tout c'est changement dans l’agreculture fransaise sont la cause de nombreu probléme environementaux et humain\n",
            "pas du tou les mure isol les jant\n",
            "\n",
            "\n",
            "\n",
            "car séte redirection et générale donc toute lentreprise dois'i plier ou démisione\n",
            "aujourd'hui on travail avec un nouveaux logiciel de sintèsse vocale pour sentrainer a lire a ecrire.\n",
            "\n",
            "\n",
            "\n",
            "nous avon discuter est osi elle ma raconté une histoire tré drol mais télmen vrais\n",
            "tue beaucoup de gens a cause des eau usagé\n",
            "\n",
            "\n",
            "\n",
            "mais l'élève même ne doit pas lésé la violence qu'il stoque a l'intérieure de lui ressortir et se diriger contre le professeur\n",
            "et me vider mes idee de la tete.\n",
            "\n",
            "\n",
            "\n",
            "Arrivé il se fait un bain de pieds pour le soulager mais malgré cela il reste énerver.\n",
            "tout ces produit chimique apré etre appliquer dans la terre sont rejeter naturelement dans les nape fréatique si que dégrade considérablement la qualiter de l’eau lui donne un tres movais gout et la rand movaise pour la santer .\n",
            "\n",
            "\n",
            "\n",
            "méme si jadmet que tu est alé dans la foret planter un arbre , pourquoi tu a mis oten de temps\n",
            "et suite a une scolariter chaotique l'élève pouré sombré dan l'illégalité et cométen de divers larcin du gore vol , viol, agression , racket,\n",
            "\n",
            "\n",
            "\n",
            "question 1 document 1;2;3\n",
            "et lit un livre tout en étant énerver\n",
            "\n",
            "\n",
            "\n",
            "a forse  il sont dégouter est finis par réélmen ne pas écouter en cour ou a ne pas trvallé suffisamment .\n",
            "aussi les gan n'utilise pas de préservatif car il en on pour insi dir pas a disposition\n",
            "\n",
            "\n",
            "\n",
            "elle est gardiniér ?\n",
            "ensuit il et un pouoir tré faible o nivau politique et n'est pas manbre prémanen du conséye de l'ONU\n",
            "\n",
            "\n",
            "\n",
            "ils sont complètement démunie et ne peuve se soigné\n",
            "Le garagiste lui a apeller un taxi et il rentra chez lui .\n",
            "\n",
            "\n",
            "\n",
            "tu ne conpren pas ?\n",
            "C’est une personne qui dort dans la voiture et qui reve qui soi a la plage\n",
            "\n",
            "\n",
            "\n",
            "En France le secteur primaire l’agriculture fait la renommer de la France aisin que sa place au nivaux mondial\n",
            "Hier je suis aller manger chez ma sœur\n",
            "\n",
            "\n",
            "\n",
            "Alors il partis pour la mer.\n",
            "Il roule tranquillement et regarde le payssage.\n",
            "\n",
            "\n",
            "\n",
            "non elle elle l'éme et la répécte plus que les autre la nuture ,\n",
            "elle néme pas les mure\n",
            "\n",
            "\n",
            "\n",
            "Sont moteur a un problème  il fume et le monsieur ce pose des question\n",
            "Il y a un monsieur qui est deriere sa voiture\n",
            "\n",
            "\n",
            "\n",
            "elle domine osi les monde des jevido avec la console sony nintendo et les jeu snk capcom et nintendo .\n",
            "il se demande se qu’il se pace et va voire sou le capot\n",
            "\n",
            "\n",
            "\n",
            "il y a aussi le fait que les gens touché par le sida en Afrique son pas pri en charge et meurt tré vite car il aucun médicament pour ne seraise que le retarder et calmer les douleur\n",
            "le bouda tré inprésioné prie pour lui est un taur blanc apré avec das sac d'or dur le dos\n",
            "\n",
            "\n",
            "\n",
            "Jack part en vacances une semaines a Bayonne\n",
            "je vais te la raconté\n",
            "\n",
            "\n",
            "\n",
            "un autre des gro probleme du japon est que il dépen des autre pays pour nourire sa sosiété en agroalimentére et osi aprovisoné et pétrole\n",
            "il essaielle de reparer sa voiture en vin.\n",
            "\n",
            "\n",
            "\n",
            "la reconversion industriel correspond a un changement de type d’industrie\n",
            "nous avons vue notre client ce matin pour lui présenté l’avent dernier version.\n",
            "\n",
            "\n",
            "\n",
            "ces cultiver sainement sans produits chimique sans pesticides ni ogm\n",
            "écoute tu ne conpren sé pas grave\n",
            "\n",
            "\n",
            "\n",
            "nous avons desherbe dans la comunnes\n",
            "grace a sa culture intensive la France a réusie a se placer a une tres bonne place au nivaus mondiale au détriment de l’écologie\n",
            "\n",
            "\n",
            "\n",
            "il tombe empannet  il essylle de la reparer\n",
            "il y a 2h30 pour allez a port leucate\n",
            "\n",
            "\n",
            "\n",
            "sony investi énormémen dargen dans la rechérche ( par exenple par rport a la france )  et est en avense sur la plus par des tecnologie si qui fait sa réusite\n",
            "et la je visite le peysage\n",
            "\n",
            "\n",
            "\n",
            "j'été dans les bois\n",
            "je doit appeler un mecanitien\n",
            "\n",
            "\n",
            "\n",
            "Je sui revenue et j’ai attaque le ménage\n",
            "voici la deuxiéme demande formé différament.\n",
            "\n",
            "\n",
            "\n",
            "J’attendit quelques minutes puis le réparateur arriva,\n",
            "je suis venue au rendez vous l'argement en avance\n",
            "\n",
            "\n",
            "\n",
            "on a grinpé en haut d'un arbre on a admiré la vue on a ri ,\n",
            "il continu a avanser\n",
            "\n",
            "\n",
            "\n",
            "il en arret maladie pendant deux semaines.\n",
            "En effet je suis disponible dès que vous souhaiterai\n",
            "\n",
            "\n",
            "\n",
            "J’appela un réparateur pour qu’il vienne la récupérée.\n",
            "Elle a fini sa vit dans une maison de retraite\n",
            "\n",
            "\n",
            "\n",
            "je suis aller a lagence d’interim a toulouse pour recuperer mes afaffaire de travail :\n",
            "malgres que quelques agriculteurs en utilises en depit de la nature,\n",
            "\n",
            "\n",
            "\n",
            "question 4 document 3\n",
            "Je suis parti au travaille et il y avait des bouchons et je me suis installe a mon poste.\n",
            "\n",
            "\n",
            "\n",
            "j ai merais faire ce metier dans ce travaile\n",
            "et la petite fille et essais   se ratraper lappareil photo\n",
            "\n",
            "\n",
            "\n",
            "puis dans une sconde partie nous étudiré les répercutions de l’agriculture\n",
            "je ne sait pas\n",
            "\n",
            "\n",
            "\n",
            "cela permetra de dininuer la polution\n",
            "et la voiture ne voulant plus démaré il decide de rentrer chez lui\n",
            "\n",
            "\n",
            "\n",
            "mais pourquoi ne vetu pas\n",
            "les région à la frontiére France - Belgique et France – allemage dont en reconversion industriel\n",
            "\n",
            "\n",
            "\n",
            "elle réspéquete est éme la nature\n",
            "je m arret sur le côter  de la route\n",
            "\n",
            "\n",
            "\n",
            "le premier contrui un pont\n",
            "casque de chantier chaussure de securiter et une paire de gants\n",
            "\n",
            "\n",
            "\n",
            "ensuite on a allumé un feu dans la cheminé et on a laisser conduire Isis qui a eu son permis il y a pas longtemps.\n",
            "je l’ai vu après qu’il soit partie\n",
            "\n",
            "\n",
            "\n",
            "ducou il rentra chez lui en la pousant.\n",
            "et alors tous le monde éme la nature ,\n",
            "\n",
            "\n",
            "\n",
            "oui sé son nom\n",
            "bizare pourtant jai fait le plein avant de parir je vais voir tout cela\n",
            "\n",
            "\n",
            "\n",
            "est un torau blanc aparé est il vien sasoir sur le deuxiem architékete ,\n",
            "et elle a appeler sa maman aui et venu la chercher au CFA\n",
            "\n",
            "\n",
            "\n",
            "je né pas a me justifier et aparamen devemen toi je narive pas aùe justifié de toute fason\n",
            "je suit party ver 9 heure juis party en bus\n",
            "\n",
            "\n",
            "\n",
            "le prix sur le marché baise\n",
            "Quand je me suie levais se matin j'ai suis aller enlever mes lentilles.\n",
            "\n",
            "\n",
            "\n",
            "mais il m’arrive des me tromper\n",
            "J'aimer beaucoup ma grans-mére.\n",
            "\n",
            "\n",
            "\n",
            "Elle a grandit pendant la geurre\n",
            "il voi une fumé sortir de sa voiture,\n",
            "\n",
            "\n",
            "\n",
            "j'aimais beaucoup ma gran mère,\n",
            "et qu'ils fodré l'utiliser pour fair dévlopais serten égion d'afrique encore coronpu ou dans la pauvreté total\n",
            "\n",
            "\n",
            "\n",
            "Cette histoire a était raconter a l'aide de c'est quatre photos.\n",
            "bon ben on dirrait quelle a rendu l ame\n",
            "\n",
            "\n",
            "\n",
            "microsoft fixerait son prix en mesur de la valeur des composen pui rajouterai une marje inportente  pour la marque\n",
            "La tige elle-même se revêt d'une dure écorce qui met le bois tendre a l'abris des injures de l'air.\n",
            "\n",
            "\n",
            "\n",
            "le taux d'urbanisation an Afrique austral approche presque de 50% de la population alors que en Afrique de l'este sela ne dépas méme pas 25 %\n",
            "control japon\n",
            "\n",
            "\n",
            "\n",
            "est le bouddah dit a larchitéque pran\n",
            "elle né pas comme les autre elle ne réste pas « cloitré » chai elle ,\n",
            "\n",
            "\n",
            "\n",
            "je vous est envoyée un message, je vous tiens au courrent à la fin de l’examin.\n",
            "maud est tré éxecentrique elle né pas comme les autre pérsone agé\n",
            "\n",
            "\n",
            "\n",
            "cetait une femme cultivé,\n",
            "elle sais marier a l'age de vingt ans ,\n",
            "\n",
            "\n",
            "\n",
            "du coup j’ai du repartir m’occupé a toulouse car je n’allais pas rentrer chez moi c’est trop loin.\n",
            "il prend une basine d eau chaud pour se rechaiffer les pied\n",
            "\n",
            "\n",
            "\n",
            "une telle culture et adapter au marcher de l’éxportation actuel\n",
            "Le club a été crée il y a un peu plus de trois ans.\n",
            "\n",
            "\n",
            "\n",
            "Pierre avai une idée,\n",
            "dan se cas en ne pela dir que en chengen ses sen le media a été corenpue\n",
            "\n",
            "\n",
            "\n",
            "est arive a tent a té rendévous\n",
            "samedi a 13 heur parti avec jean et sont frere ont a fai une parti d airsoft a l hopita pour teste ma nouvelle réplique pour voir le posibilité que javais\n",
            "\n",
            "\n",
            "\n",
            "mais quelque métres plus loin il tonba en panne\n",
            "si sé sela sé comme tu veu mais alors ne me parle plu d'elle ,\n",
            "\n",
            "\n",
            "\n",
            "Le soir dés qu'il entend mon pére arriver même qu'il dorme il va descendre lui dire bonjours en miaulent.\n",
            "C’est l’histoire d’un bonhomme qui voulais aller à la mer pendant ces vacances.\n",
            "\n",
            "\n",
            "\n",
            "je te croiyé plus sage\n",
            "et pour les mure\n",
            "\n",
            "\n",
            "\n",
            "et puis il a du appeler un reparateur pour repere sa voiture\n",
            "elle ma raconté une histoir\n",
            "\n",
            "\n",
            "\n",
            "la plus par des jen save se contrôler donc la violence n'est pas une fataliter .\n",
            "Reconversion industriel\n",
            "\n",
            "\n",
            "\n",
            "la violence n'est pas une fataliter .\n",
            "pourtant on ne voi pas tou les jour une agression o milieu de la rue a cause d'une personne qui oré insulter ou énervé une autre personne ;\n",
            "\n",
            "\n",
            "\n",
            "elle a fini sa vie dans uen maison de retraite.\n",
            "il et enerver cher lui il lis son livre .\n",
            "\n",
            "\n",
            "\n",
            "les pon pérméte d'alé dans dé endroi ou il nalé pas aven apr peur de travérsé la riviére ou le fleuve\n",
            "Quand mes parents sont rentrés de voyage mon chaton les a lécher sur toute la figure .\n",
            "\n",
            "\n",
            "\n",
            "toi qui néme pas  tenuiyé tu dit que tu est monté dans un arbre est voilà sé toi tu a regardé la vu panden 2h\n",
            "Ce matin, je me suis préparer pour aller en cours et j’ai pris les transports en commun pour cela.\n",
            "\n",
            "\n",
            "\n",
            "il trouve pas le probleme\n",
            "et la fille pend un foto de son père tonbé.\n",
            "\n",
            "\n",
            "\n",
            "et finir même par se dirigé contre un camarade de classe ou une personne quelconque de leur entourage ,\n",
            "ce seisme na pas etes aussi violent que le précedent\n",
            "\n",
            "\n",
            "\n",
            "Une fois rentré il sa soi sur le canapé\n",
            "et a l’hopital de la ville il ya eu beaucoup de monde,\n",
            "\n",
            "\n",
            "\n",
            "Séte maud quelle agé avé télle tu dit\n",
            "Le garagiste le plus proche étais à coté de chez lui.\n",
            "\n",
            "\n",
            "\n",
            "la suprétion des hais qui délimite les exploitation empéche ausemploiyer de voire les c'est limite ce qui donne a ces enploiyer une inprésion de terrin a perte de vue et est tres dépriment l’aure de leur travaille\n",
            "C'étais une femme cultiver, bien viellante et toujours de bonne humuer.\n",
            "\n",
            "\n",
            "\n",
            "il ne peu osi cré pour insidir ocune armé et na pas le nuclérée\n",
            "Je le suivie dans sa fourgonette\n",
            "\n",
            "\n",
            "\n",
            "sof biensur dan le cas ou sanéte coquet privet conserne un délit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chatGPT_error_offsets(annotated_segment):\n",
        "  sent = annotated_segment[\"sentence\"]\n",
        "\n",
        "  current_offset = 0\n",
        "  error_offsets = []\n",
        "  \n",
        "\n",
        "  for token in annotated_segment[\"tokens\"]:\n",
        "    # check index of token (0 if no whitespace, 1 if whitespace before token)\n",
        "    # sometimes, chatGPT corrected erronous tokens instead of just labelling, then the substring doesn't match with the token in the source sentence\n",
        "    if token[0] not in sent:\n",
        "      error_offsets.append((current_offset + len(sent) - len(sent.lstrip())))\n",
        "      continue\n",
        "    index = sent.index(token[0])\n",
        "    # add index to current_offset\n",
        "    current_offset += index\n",
        "    # add offset to error_offsets if the token is labelled as \"err\"\n",
        "    if token[1] == \"err\":\n",
        "      error_offsets.append(current_offset)\n",
        "    # add length of token to current_offset\n",
        "    current_offset += len(token[0])\n",
        "    # cut token from input sentence\n",
        "    sent = sent[index:].lstrip().lstrip(token[0])\n",
        "  \n",
        "  return error_offsets\n",
        "    "
      ],
      "metadata": {
        "id": "R6DQPczhEUm3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_annotations_chatGPT(path):\n",
        "  prepared_data = []\n",
        "  annotated_segs = []\n",
        "  with open(path, 'r', encoding='utf-8') as inf:\n",
        "    for line in inf:\n",
        "      annotated_segs.append(json.loads(line))\n",
        "\n",
        "  # TODO: \n",
        "  for seg in annotated_segs:\n",
        "    # tokenize segment\n",
        "    tokenized_seg = tokenize_with_offsets(seg[\"sentence\"])\n",
        "\n",
        "    # TODO: get offsets for errors\n",
        "    error_offsets = get_chatGPT_error_offsets(seg)\n",
        "    # TODO: set labels for each token\n",
        "    for i, token in enumerate(tokenized_seg):\n",
        "        if token.char_offset in error_offsets:\n",
        "          tokenized_seg[i].set_label(\"err\")\n",
        "        else:\n",
        "          tokenized_seg[i].set_label(\"corr\")\n",
        "    \n",
        "    prepared_data.append(tokenized_seg)\n",
        "\n",
        "  return prepared_data\n"
      ],
      "metadata": {
        "id": "6Cm_fKkpuzUJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Typewise and ChatGPT\n",
        "\n",
        "### Compute Performance in Terms of P/R/F1/ACC"
      ],
      "metadata": {
        "id": "qChe6IDKgQCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stats(gold_data, test_data):\n",
        "  total_n = 0\n",
        "  tp, fp, tn, fn = 0, 0, 0, 0\n",
        "  for gold, test in zip(gold_data, test_data):\n",
        "    for gold_token, test_token in zip(gold, test):\n",
        "      total_n += 1\n",
        "      if gold_token.label == \"err\" and test_token.label == \"err\":\n",
        "        tp += 1\n",
        "      elif gold_token.label == \"corr\" and test_token.label == \"corr\":\n",
        "        tn += 1\n",
        "      elif gold_token.label == \"corr\" and test_token.label == \"err\":\n",
        "        fp += 1\n",
        "      elif gold_token.label == \"err\" and test_token.label == \"corr\":\n",
        "        fn += 1\n",
        "\n",
        "  try:\n",
        "    precision = tp / (tp + fp)\n",
        "  except ZeroDivisionError:\n",
        "    precision = 0\n",
        "  try:\n",
        "    recall = tp / (tp + fn)\n",
        "  except ZeroDivisionError:\n",
        "    recall = 0\n",
        "  try:\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
        "  except ZeroDivisionError:\n",
        "    f1 = 0\n",
        "  try:\n",
        "    acc = (tp + tn) / total_n\n",
        "  except ZeroDivisionError:\n",
        "    acc = 0\n",
        "\n",
        "  return precision, recall, f1, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "5bU8V9qS5Fcp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_percentages(gold_counts, test_counts):\n",
        "  percentages = {}\n",
        "  for key, gold_count in gold_counts.items():\n",
        "    test_count = test_counts[key]\n",
        "    perc = test_count / gold_count * 100\n",
        "    percentages[key] = perc\n",
        "  return percentages"
      ],
      "metadata": {
        "id": "OQjvYQvSjL2_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data for evaluation\n",
        "typewise_data = prepare_annotations_typewise('data/typewise_annotated_correction.json', 'data/typewise_annotated_grammar.json')\n",
        "chatGPT_data = prepare_annotations_chatGPT(\"data/annotated_chatGPT_clean.jsonl\")\n",
        "gold_data  = prepare_gold_data(test)"
      ],
      "metadata": {
        "id": "iRtauoYwY3up"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute stats for both systems\n",
        "typewise_p, typewise_r, typewise_f1, typewise_acc = compute_stats(gold_data, typewise_data)\n",
        "print(\"Typewise\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"Precision: {typewise_p}\\nRecall: {typewise_r}\\nF1: {typewise_f1}\\nAccuracy: {typewise_acc}\")\n",
        "print(\"=============================\\n\\n\")\n",
        "\n",
        "\n",
        "chatGPT_p, chatGPT_r, chatGPT_f1, chatGPT_acc = compute_stats(gold_data, chatGPT_data)\n",
        "print(\"ChatGPT\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"Precision: {chatGPT_p}\\nRecall: {chatGPT_r}\\nF1: {chatGPT_f1}\\nAccuracy: {chatGPT_acc}\")\n",
        "print(\"=============================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbFo-KjSFLhZ",
        "outputId": "73b1f33c-60be-46ed-b052-3f3ab2ec14a7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Typewise\n",
            "-----------------------------\n",
            "Precision: 0.652542372881356\n",
            "Recall: 0.719626168224299\n",
            "F1: 0.6844444444444445\n",
            "Accuracy: 0.8301435406698564\n",
            "=============================\n",
            "\n",
            "\n",
            "ChatGPT\n",
            "-----------------------------\n",
            "Precision: 0.7394270122783083\n",
            "Recall: 0.7265415549597856\n",
            "F1: 0.7329276538201487\n",
            "Accuracy: 0.8645869043537882\n",
            "=============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Misses and Hits per Error Type\n",
        "\n",
        "####Error Categorization according to Bodard et al 2022:\n",
        "\n",
        "Mapping to annotation guide of corpus DYS:\n",
        "\n",
        "- Wrong grapheme, silent letters and lexical morphograms: lettre muette + phonétisation\n",
        "- Inflection errors: accord + conjugaison\n",
        "- Confusion between graphemes phonetically close: approximation\n",
        "- Homophones: homophones\n",
        "- Split word or run-on: segmentation + apostrophe + trait d'union\n",
        "- Capitalization: majuscule\n",
        "- Omission: omission\n",
        "- Substitution: substitution\n",
        "- Word omission or repetition: omission mot + ajout mot + déplacement mot\n",
        "- Addition: ajout\n",
        "- Transposition: transposition\n",
        "- Wrong lexical choice: substitution mots\n",
        "- Liaison: liaison\n",
        "- Unrecognized word: mot non reconnu\n",
        "- Shifting: déplacement\n",
        "\n"
      ],
      "metadata": {
        "id": "194Y4zG7PCq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ERROR_TYPES = {\n",
        "    \"lettre muette\": \"Wrong grapheme, silent letters and lexical morphograms\",\n",
        "    \"phonétisation\": \"Wrong grapheme, silent letters and lexical morphograms\",\n",
        "    \"accord\": \"Inflection errors\",\n",
        "    \"conjugaison\": \"Inflection errors\",\n",
        "    \"approximation\": \"Confusion between graphemes phonetically close\",\n",
        "    \"homophones\": \"Homophones\",\n",
        "    \"segmentation\": \"Split word or run-on\",\n",
        "    \"apostrophe\": \"Split word or run-on\",\n",
        "    \"trait d'union\": \"Split word or run-on\",\n",
        "    \"majuscule\": \"Capitalization\",\n",
        "    \"omission\": \"Omission\",\n",
        "    \"substitution\": \"Substitution\",\n",
        "    \"omission mot\": \"Word omission or repetition\",\n",
        "    \"ajout mot\": \"Word omission or repetition\",\n",
        "    \"déplacement mot\": \"Word omission or repetition\",\n",
        "    \"ajout\": \"Addition\",\n",
        "    \"transposition\": \"Transposition\",\n",
        "    \"substitution mots\": \"Wrong lexical choice\",\n",
        "    \"liaison\": \"Liaison\",\n",
        "    \"mot non reconnu\": \"Unrecognized word\",\n",
        "    \"déplacement\": \"Shifting\"\n",
        "}"
      ],
      "metadata": {
        "id": "XIuXF0N0pycd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_all_error_types(path):\n",
        "  df = pd.read_csv(path)\n",
        "\n",
        "  # Group the dataframe by the cleaned sentences and keep all the original rows\n",
        "  grouped = df.groupby(\"Type de l'erreur\")\n",
        "  names = []\n",
        "  # Iterate over the grouped dataframe\n",
        "  for name, group in grouped:\n",
        "    for row in group.iterrows():\n",
        "      names.append(name)\n",
        "  \n",
        "  return Counter(names)\n",
        "\n",
        "error_types = get_all_error_types(\"data/annotation_erreurs_corpus_dys.csv\")\n",
        "total_errs = 0\n",
        "for err, count in error_types.items():\n",
        "  print(err, count)\n",
        "  total_errs += count\n",
        "print(len(error_types))\n",
        "print(total_errs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSxQfEmcUJdi",
        "outputId": "b44797c6-ef9e-4454-fb36-a0eb777fcf0c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accord 534\n",
            "ajout 37\n",
            "ajout mot 10\n",
            "apostrophe 117\n",
            "approximation 473\n",
            "conjugaison 260\n",
            "déplacement 2\n",
            "déplacement mot 1\n",
            "homophones 334\n",
            "lettre muette 251\n",
            "liaison 6\n",
            "majuscule 93\n",
            "mot non reconnu 5\n",
            "omission 90\n",
            "omission mot 27\n",
            "phonétisation 556\n",
            "segmentation 47\n",
            "substitution 47\n",
            "substitution mots 17\n",
            "trait d'union 24\n",
            "transposition 30\n",
            "21\n",
            "2961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error analysis for typewise & chatGPT\n",
        "def analyze_errors(gold_data, test_data):\n",
        "  fn_error_types = []\n",
        "  tp_error_types = []\n",
        "  fn, tp = 0, 0\n",
        "\n",
        "  for gold, test in zip(gold_data, test_data):\n",
        "    for gold_token, test_token in zip(gold, test):\n",
        "      # hits\n",
        "      if gold_token.label == \"err\" and test_token.label == \"err\":\n",
        "        tp += 1\n",
        "        tp_error_types.extend([ERROR_TYPES[err] for err in gold_token.types])\n",
        "      # misses\n",
        "      elif gold_token.label == \"err\" and test_token.label == \"corr\":\n",
        "        fn += 1\n",
        "        fn_error_types.extend([ERROR_TYPES[err] for err in gold_token.types])\n",
        "\n",
        "  fn_type_count = Counter(fn_error_types)\n",
        "  tp_type_count = Counter(tp_error_types)\n",
        "\n",
        "  return fn_type_count, tp_type_count, fn, tp"
      ],
      "metadata": {
        "id": "djEJ2WQ66zpx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_error_types_in_prepared_gold_data(data):\n",
        "  error_types = []\n",
        "  for tok_sent in data:\n",
        "    for tok in tok_sent:\n",
        "      if tok.label == \"err\":\n",
        "        error_types.extend([ERROR_TYPES[err] for err in tok.types])\n",
        "  err_type_count = Counter(error_types)\n",
        "  return err_type_count\n"
      ],
      "metadata": {
        "id": "jqm3crLd-i68"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counted_error_types_gold = get_error_types_in_prepared_gold_data(gold_data)\n",
        "for t, c in counted_error_types_gold.items():\n",
        "  print(f\"{t}: {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyW1rLIZiuey",
        "outputId": "5fd51f23-3ee0-42c2-c161-bd54e9689882"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homophones: 103\n",
            "Confusion between graphemes phonetically close: 183\n",
            "Inflection errors: 269\n",
            "Wrong grapheme, silent letters and lexical morphograms: 316\n",
            "Word omission or repetition: 13\n",
            "Capitalization: 41\n",
            "Transposition: 14\n",
            "Omission: 33\n",
            "Split word or run-on: 73\n",
            "Substitution: 25\n",
            "Addition: 9\n",
            "Wrong lexical choice: 6\n",
            "Liaison: 2\n",
            "Shifting: 1\n",
            "Unrecognized word: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typewise_fn_type_count, typewise_tp_type_count, fn, tp = analyze_errors(gold_data, typewise_data)\n",
        "\n",
        "print(f\"Number of Misses by Typewise per Error Type\")\n",
        "print(f\"-------------------------------------------\")\n",
        "for key, count in typewise_fn_type_count.items():\n",
        "  print(f\"{key}: {count}\")\n",
        "print(f\"-------------------------------------------\")\n",
        "\n",
        "print(f\"FNs: {typewise_fn_type_count}\")\n",
        "print(f\"TPs: {typewise_tp_type_count}\")\n",
        "print(f\"#FNs: {fn}\")\n",
        "print(f\"#TPs: {tp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz_BDdGtyPil",
        "outputId": "b086d7b2-6165-4864-df9a-521dc3f9214e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misses by Typewise per Error Type\n",
            "-------------------------------------------\n",
            "Homophones: 45\n",
            "Word omission or repetition: 5\n",
            "Inflection errors: 100\n",
            "Confusion between graphemes phonetically close: 24\n",
            "Transposition: 4\n",
            "Substitution: 11\n",
            "Wrong grapheme, silent letters and lexical morphograms: 41\n",
            "Split word or run-on: 5\n",
            "Wrong lexical choice: 5\n",
            "Capitalization: 2\n",
            "Addition: 2\n",
            "Omission: 2\n",
            "Unrecognized word: 1\n",
            "-------------------------------------------\n",
            "FNs: Counter({'Inflection errors': 100, 'Homophones': 45, 'Wrong grapheme, silent letters and lexical morphograms': 41, 'Confusion between graphemes phonetically close': 24, 'Substitution': 11, 'Word omission or repetition': 5, 'Split word or run-on': 5, 'Wrong lexical choice': 5, 'Transposition': 4, 'Capitalization': 2, 'Addition': 2, 'Omission': 2, 'Unrecognized word': 1})\n",
            "TPs: Counter({'Wrong grapheme, silent letters and lexical morphograms': 275, 'Inflection errors': 169, 'Confusion between graphemes phonetically close': 159, 'Split word or run-on': 68, 'Homophones': 58, 'Capitalization': 39, 'Omission': 31, 'Substitution': 14, 'Transposition': 10, 'Word omission or repetition': 8, 'Addition': 7, 'Unrecognized word': 3, 'Liaison': 2, 'Wrong lexical choice': 1, 'Shifting': 1})\n",
            "#FNs: 210\n",
            "#TPs: 539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typewise_fn_perc = get_percentages(counted_error_types_gold, typewise_fn_type_count)\n",
        "\n",
        "ordered = dict(sorted(typewise_fn_perc.items(), key=lambda item: item[1], reverse=True))\n",
        "for key, perc in ordered.items():\n",
        "  print(f\"{key}: {perc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXbYV-DZkWWP",
        "outputId": "4dc916bc-50fb-4e67-9f35-120ad0498f76"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong lexical choice: 83.33333333333334\n",
            "Substitution: 44.0\n",
            "Homophones: 43.689320388349515\n",
            "Word omission or repetition: 38.46153846153847\n",
            "Inflection errors: 37.174721189591075\n",
            "Transposition: 28.57142857142857\n",
            "Unrecognized word: 25.0\n",
            "Addition: 22.22222222222222\n",
            "Confusion between graphemes phonetically close: 13.114754098360656\n",
            "Wrong grapheme, silent letters and lexical morphograms: 12.974683544303797\n",
            "Split word or run-on: 6.8493150684931505\n",
            "Omission: 6.0606060606060606\n",
            "Capitalization: 4.878048780487805\n",
            "Liaison: 0.0\n",
            "Shifting: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "typewise_tp_perc = get_percentages(counted_error_types_gold, typewise_tp_type_count)\n",
        "\n",
        "ordered = dict(sorted(typewise_tp_perc.items(), key=lambda item: item[1], reverse=True))\n",
        "for key, perc in ordered.items():\n",
        "  print(f\"{key}: {perc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x-qSSuYlili",
        "outputId": "44e0c442-991e-4007-9bcd-2264b93ebfd2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liaison: 100.0\n",
            "Shifting: 100.0\n",
            "Capitalization: 95.1219512195122\n",
            "Omission: 93.93939393939394\n",
            "Split word or run-on: 93.15068493150685\n",
            "Wrong grapheme, silent letters and lexical morphograms: 87.0253164556962\n",
            "Confusion between graphemes phonetically close: 86.88524590163934\n",
            "Addition: 77.77777777777779\n",
            "Unrecognized word: 75.0\n",
            "Transposition: 71.42857142857143\n",
            "Inflection errors: 62.825278810408925\n",
            "Word omission or repetition: 61.53846153846154\n",
            "Homophones: 56.310679611650485\n",
            "Substitution: 56.00000000000001\n",
            "Wrong lexical choice: 16.666666666666664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatGPT_fn_type_count, chatGPT_tp_type_count, fn, tp = analyze_errors(gold_data, chatGPT_data)\n",
        "\n",
        "print(f\"Number of Misses by ChatGPT per Error Type\")\n",
        "print(f\"-------------------------------------------\")\n",
        "for key, count in chatGPT_fn_type_count.items():\n",
        "  print(f\"{key}: {count}\")\n",
        "print(f\"-------------------------------------------\")\n",
        "\n",
        "print(f\"FNs: {chatGPT_fn_type_count}\")\n",
        "print(f\"TPs: {chatGPT_tp_type_count}\")\n",
        "print(f\"#FNs: {fn}\")\n",
        "print(f\"#TPs: {tp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SYO-CZ7h5_T",
        "outputId": "80452d4c-f996-4013-e40c-cd7ce6cdd8f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Misses by ChatGPT per Error Type\n",
            "-------------------------------------------\n",
            "Homophones: 43\n",
            "Inflection errors: 79\n",
            "Confusion between graphemes phonetically close: 33\n",
            "Capitalization: 27\n",
            "Split word or run-on: 13\n",
            "Wrong grapheme, silent letters and lexical morphograms: 21\n",
            "Wrong lexical choice: 5\n",
            "Word omission or repetition: 3\n",
            "Substitution: 4\n",
            "Addition: 1\n",
            "Transposition: 1\n",
            "-------------------------------------------\n",
            "FNs: Counter({'Inflection errors': 79, 'Homophones': 43, 'Confusion between graphemes phonetically close': 33, 'Capitalization': 27, 'Wrong grapheme, silent letters and lexical morphograms': 21, 'Split word or run-on': 13, 'Wrong lexical choice': 5, 'Substitution': 4, 'Word omission or repetition': 3, 'Addition': 1, 'Transposition': 1})\n",
            "TPs: Counter({'Wrong grapheme, silent letters and lexical morphograms': 293, 'Inflection errors': 189, 'Confusion between graphemes phonetically close': 150, 'Homophones': 60, 'Split word or run-on': 60, 'Omission': 33, 'Substitution': 21, 'Capitalization': 14, 'Transposition': 13, 'Word omission or repetition': 10, 'Addition': 8, 'Unrecognized word': 4, 'Liaison': 2, 'Wrong lexical choice': 1, 'Shifting': 1})\n",
            "#FNs: 204\n",
            "#TPs: 542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatGPT_fn_perc = get_percentages(counted_error_types_gold, chatGPT_fn_type_count)\n",
        "\n",
        "ordered = dict(sorted(chatGPT_fn_perc.items(), key=lambda item: item[1], reverse=True))\n",
        "for key, perc in ordered.items():\n",
        "  print(f\"{key}: {perc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB6-1tSak9gF",
        "outputId": "e2c1591d-e9cb-404a-97dd-2f5d0775d246"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong lexical choice: 83.33333333333334\n",
            "Capitalization: 65.85365853658537\n",
            "Homophones: 41.74757281553398\n",
            "Inflection errors: 29.36802973977695\n",
            "Word omission or repetition: 23.076923076923077\n",
            "Confusion between graphemes phonetically close: 18.0327868852459\n",
            "Split word or run-on: 17.80821917808219\n",
            "Substitution: 16.0\n",
            "Addition: 11.11111111111111\n",
            "Transposition: 7.142857142857142\n",
            "Wrong grapheme, silent letters and lexical morphograms: 6.645569620253164\n",
            "Omission: 0.0\n",
            "Liaison: 0.0\n",
            "Shifting: 0.0\n",
            "Unrecognized word: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatGPT_tp_perc = get_percentages(counted_error_types_gold, chatGPT_tp_type_count)\n",
        "\n",
        "ordered = dict(sorted(chatGPT_tp_perc.items(), key=lambda item: item[1], reverse=True))\n",
        "for key, perc in ordered.items():\n",
        "  print(f\"{key}: {perc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5cJCp7n6oy",
        "outputId": "3a9cdf09-b9f5-496b-eb05-355332997e32"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Omission: 100.0\n",
            "Liaison: 100.0\n",
            "Shifting: 100.0\n",
            "Unrecognized word: 100.0\n",
            "Transposition: 92.85714285714286\n",
            "Wrong grapheme, silent letters and lexical morphograms: 92.72151898734177\n",
            "Addition: 88.88888888888889\n",
            "Substitution: 84.0\n",
            "Split word or run-on: 82.1917808219178\n",
            "Confusion between graphemes phonetically close: 81.9672131147541\n",
            "Word omission or repetition: 76.92307692307693\n",
            "Inflection errors: 70.26022304832715\n",
            "Homophones: 58.252427184466015\n",
            "Capitalization: 34.146341463414636\n",
            "Wrong lexical choice: 16.666666666666664\n"
          ]
        }
      ]
    }
  ]
}